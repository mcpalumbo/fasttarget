from ftscripts import programs, metadata, files
import os
import shutil
import sys
import re
import numpy
import pandas as pd
import networkx as nx
from networkx.algorithms.centrality.betweenness import betweenness_centrality
from networkx.algorithms.components.connected import connected_components

# Functions to process Pathway Tools output files

def process_sbml(base_path, organism_name, sbml_file):
    """
    Process the SBML file to generate the Ubiquitous Compounds file inside the metabolism directory.
    This function uses the `run_ubiquitous` function from the `programs` module.

    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.
    :param sbml_file: Path to the SBML file
    """
    
    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')
    sbml_path = os.path.join(metabolism_dir, f'{organism_name}.sbml')

    if os.path.exists(sbml_file):
        shutil.copyfile(sbml_file, sbml_path)

        print(f'Processing {sbml_path}')

        programs.run_ubiquitous(sbml_path, metabolism_dir)

        print(f'List of Ubiquitous Compounds saved to {metabolism_dir}/ubiquitous_compounds.txt.')
 
    else:
        print(f"SBML file '{sbml_file}' not found.", file=sys.stderr)

def ubiquitous_checker(base_path, organism_name):
    """
    Check if the ubiquitous_compounds.txt file was renamed and moved to the metabolism directory.
    This function prompts the user to curate and rename the file and waits for the user to press Enter.    

    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.
    """

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')
    ubiquitous_file = os.path.join(metabolism_dir, f'{organism_name}_ubiquitous.txt')
    
    print(f'Please curate ubiquitous_compounds.txt and rename as {ubiquitous_file}')
    
    while True:
        input("Press Enter after renaming the file...")
        
        if os.path.exists(ubiquitous_file):
            print(f"File successfully renamed to {ubiquitous_file}. Proceeding with the pipeline...")
            break
        else:
            print(f"{ubiquitous_file} not found. Please rename the file or try again.")

def generate_sif(base_path, organism_name):
    """
    Generate the network.sif file from the SBML file and the Ubiquitous Compounds file.
    This function uses the `run_sbml_to_sif` function from the `programs` module.
    
    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.
    """

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')
    sbml_path = os.path.join(metabolism_dir, f'{organism_name}.sbml')
    ubiquitous_file = os.path.join(metabolism_dir, f'{organism_name}_ubiquitous.txt')

    programs.run_sbml_to_sif(sbml_path, ubiquitous_file, metabolism_dir)
    print(f'Sif file generated in {metabolism_dir}.')

def process_chokepoint_ptools(base_path, organism_name, chokepoint_file):
    """
    Process the chokepoint file generated by Pathway Tools.
    This function reads the chokepoint file, extracts the producing and consuming chokepoints, and returns the sets of reactions.
    
    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.
    :param chokepoint_file: Path to the chokepoint file.

    :return: Sets of producing chokepoints.
    :return: Sets of consuming chokepoints.
    :return: Sets of both producing and consuming chokepoints.
    """

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')
    chokepoint_path = os.path.join(metabolism_dir, f'{organism_name}_chokepoints.txt')

    if os.path.exists(chokepoint_file):
        shutil.copy(chokepoint_file, chokepoint_path)
    else:
        print(f"Chokepoint file '{chokepoint_file}' not found.", file=sys.stderr)

    with open(chokepoint_path, 'r') as file:
        text = file.read()

    # Define regular expressions for finding sections
    producing_pattern = r"Reactions that are chokepoints on the producing side \(\d+\):\n((?:.*\n)*)\nReactions that are chokepoints on the consuming side"
    consuming_pattern = r"Reactions that are chokepoints on the consuming side \(\d+\):\n((?:.*\n)*)\nReactions that are not chokepoints, but are candidates"

    # Extract the producing side reactions
    producing_match = re.search(producing_pattern, text)
    producing_reactions = producing_match.group(1).strip().split('\n') if producing_match else []

    # Extract the consuming side reactions
    consuming_match = re.search(consuming_pattern, text)
    consuming_reactions = consuming_match.group(1).strip().split('\n') if consuming_match else []

    producing_list = []
    for reaction in producing_reactions:
        producing_list.append(reaction.split('  ', 1)[0])
        
    consuming_list = []
    for reaction in consuming_reactions:
        consuming_list.append(reaction.split('  ', 1)[0])
    
    producing_set = set(producing_list)
    consuming_set = set(consuming_list)
    
    print('Total producing chokepoints')
    print(len(producing_set))
    print('Total consuming chokepoints')
    print(len(consuming_set))
    
    both_set = producing_set & consuming_set
    producing_only_set = producing_set - both_set
    consuming_only_set = consuming_set - both_set
    
    print('Only producing chokepoints')
    print(len(producing_only_set))
    print('Only consuming chokepoints')
    print(len(consuming_only_set))
    print('Both producing and consuming chokepoints')
    print(len(both_set))
    
    return producing_only_set, consuming_only_set, both_set

def process_genes_smarttable(base_path, organism_name, smarttable_file):
    """
    This function reads the Smarttable file, extracts the genes and their associated reactions, and returns a dictionary with the genes as keys and the reactions as values.
    
    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.
    :param smarttable_file: Path to the Smarttable file.
    
    :return: Dictionary with the genes as keys and the reactions as values.
    """

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')
    smarttable_path = os.path.join(metabolism_dir, f'{organism_name}_smarttable.txt')

    if os.path.exists(smarttable_file):
        shutil.copy(smarttable_file, smarttable_path)
    else:
        print(f"Smarttable file '{smarttable_file}' not found.", file=sys.stderr)

    df_genes = pd.read_csv(smarttable_path, delimiter='\t', usecols=['ID', 'Reactions'])
    df_genes = df_genes.dropna(subset=['Reactions'], how='all')
    reaction_dict = {row['ID']: [reaction.strip() for reaction in row['Reactions'].split(',')] for index, row in df_genes.iterrows()}

    return reaction_dict

def write_sif(graph, file_path):
    """
    Write a networkx graph to a SIF file.

    :param graph: Networkx graph.
    :param file_path: Path to the SIF file.
    """
    with open(file_path, 'w') as file:
        for edge in graph.edges():
            file.write(f"{edge[0]}\tinteracts\t{edge[1]}\n")

def read_sif(base_path, organism_name):
    """
    This function reads the network.sif file, generates the principal component of the network, and calculates the betweenness centrality and node degrees.

    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.

    :return: The complete network.
    :return: The principal component of the network.
    :return: Dictionary with the betweenness centrality values.
    :return: Dictionary with the node degrees.
    """

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')
    sif_path = os.path.join(metabolism_dir, 'network.sif')

    G = nx.DiGraph()
    with open(sif_path, 'r') as file:
        for line in file:
            parts = line.strip().split()
            if len(parts) == 3:
                source, interaction, target = parts
                G.add_edge(source, target, interaction=interaction)
                
    largest_component = max(nx.weakly_connected_components(G), key=len)
    subgraph = G.subgraph(largest_component).copy()
    write_sif(subgraph, os.path.join(metabolism_dir, f'{organism_name}_principal_component.sif'))
    
    betweenness_centrality = nx.betweenness_centrality(subgraph)
    max_value = max(value for value in betweenness_centrality.values())
    normalized_betweenness_centrality = {key: (value / max_value) for key, value in betweenness_centrality.items()} 

    node_degrees = dict(subgraph.degree())
    
    return G, subgraph, normalized_betweenness_centrality, node_degrees   

def mapping_metabolism(locus_reaction_dict, reaction_property_dict):
    """
    This function maps the betweenness centrality values to the genes.

    :param locus_reaction_dict: Dictionary with the genes as keys and the reactions as values.
    :param reaction_property_dict: Dictionary with the reactions as keys and the betweenness centrality values as values.

    :return: Dictionary with the genes as keys and the betweenness centrality values as values.
    """
    
    locustag_to_values = {}

    for gene, rxns in locus_reaction_dict.items():
        values = [reaction_property_dict[rxn] for rxn in rxns if rxn in reaction_property_dict]
        locustag_to_values[gene] = values
        
    locustag_max = {}
    for gene, values in locustag_to_values.items():
        if len(values)>0:
            locustag_max[gene] = max(values)
        else:
            locustag_max[gene] = 0   
    
    return locustag_max

def mapping_chokepoints(locus_reaction_dict, producing_ck, consuming_ck, both_ck):
    """
    This function maps the chokepoints to the genes.

    :param locus_reaction_dict: Dictionary with the genes as keys and the reactions as values.
    :param producing_ck: Set of producing chokepoints.
    :param consuming_ck: Set of consuming chokepoints.
    :param both_ck: Set of both producing and consuming chokepoints.

    :return: Dictionaries with the genes as keys and the chokepoints as values. Returns three dictionaries: producing chokepoints, consuming chokepoints, and both chokepoints.
    """
    
    producing_chokepoint = {}
    consuming_chokepoint = {}
    both_chokepoint = {}
        
    for gene, rxns in locus_reaction_dict.items():
        for rxn in rxns:
            if rxn in producing_ck:
                producing_chokepoint[gene] = rxn
            elif rxn in consuming_ck:
                consuming_chokepoint[gene] = rxn
            elif rxn in both_ck:
                both_chokepoint[gene] = rxn

    return producing_chokepoint, consuming_chokepoint, both_chokepoint

def run_metabolism_ptools (base_path, organism_name, sbml_file, chokepoint_file, smarttable_file):
    """
    Runs the metabolism pipeline. 
    It processes the SBML file, generates the Ubiquitous Compounds file, the network SIF file, and the gene-reaction mapping. 
    Returns DataFrames with locus_tag as key and the values of betweenness centrality, node degrees, and chokepoints.
    The DataFrames are created using the functions `metadata_table_with_values` and `metadata_table_with_values` from the `metadata` module.   

    :param base_path: Base path where the repository data is stored.
    :param organism_name: Name of the organism.
    :param sbml_file: Path to the SBML file.
    :param chokepoint_file: Path to the chokepoint file.
    :param smarttable_file: Path to the Smarttable file.

    :return: DataFrame with the betweenness centrality values.
    :return: DataFrame with the node degrees.
    :return: DataFrame with the chokepoints.
    """

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')

    print(f'---------- 1. Processing SBML file ----------')
    process_sbml(base_path, organism_name, sbml_file)
    print(f'---------- 1. Finished ----------')

    print(f'---------- 2. Generating Ubiquitous compounds file ----------')
    ubiquitous_checker(base_path, organism_name)
    print(f'---------- 2. Finished ----------')

    print(f'---------- 3. Generating network SIF file ----------')
    generate_sif(base_path, organism_name)
    print(f'---------- 3. Finished ----------')

    print(f'---------- 4. Map proteins and reactions ----------')
    locus_reaction_dict = process_genes_smarttable(base_path, organism_name, smarttable_file)
    print(f'---------- 4. Finished ----------')

    print(f'---------- 5. Calculate Centrality ----------')
    graph, subgraph, reaction_centrality_dict, reaction_edges_dict = read_sif(base_path, organism_name)
    bc_dict = mapping_metabolism(locus_reaction_dict, reaction_centrality_dict)
    edge_dict = mapping_metabolism(locus_reaction_dict, reaction_edges_dict)
    print(f'---------- 5. Finished ----------')

    print(f'---------- 6. Obtaining Choke-points ----------')
    producing_set, consuming_set, both_set = process_chokepoint_ptools(base_path, organism_name, chokepoint_file)
    producing_chokepoint, consuming_chokepoint, both_chokepoint = mapping_chokepoints(locus_reaction_dict, producing_set, consuming_set, both_set)
    print(f'---------- 6. Finished ----------')

    print(f'---------- 7. Generating results files ----------')
    bcentrality_df = metadata.metadata_table_with_values(base_path, organism_name, bc_dict, "PTOOLS_betweenness_centrality", metabolism_dir, 0)
    edges_df = metadata.metadata_table_with_values(base_path, organism_name, edge_dict, "PTOOLS_edges", metabolism_dir, 0)
    producing_df = metadata.metadata_table_with_values(base_path, organism_name, producing_chokepoint, "PTOOLS_producing_chokepoints", metabolism_dir, 'None')
    consuming_df = metadata.metadata_table_with_values(base_path, organism_name, consuming_chokepoint, "PTOOLS_consuming_chokepoints", metabolism_dir, 'None')
    both_df = metadata.metadata_table_with_values(base_path, organism_name, both_chokepoint, "PTOOLS_both_chokepoints", metabolism_dir, 'None')
    print(f'---------- 7. Finished ----------')

    return bcentrality_df, edges_df, producing_df, consuming_df, both_df

# Functions for process an external SBML with MetaGraphTools (docker)

def process_external_sbml(sbml_file, output_path, filter_file=None):

    """
    Process an external SBML file using MetaGraphTools Docker image to generate the Ubiquitous Compounds file.

    :param sbml_file: Path to the SBML file.
    :param output_path: Path to the output directory.
    :param filter_file: Optional path to the filter file for ubiquitous compounds.
    """

    if not os.path.exists(output_path):
        os.makedirs(output_path, exist_ok=True)

    # Move SBML file to output directory
    sbml_basename = os.path.basename(sbml_file)
    sbml_file_destination = os.path.join(output_path, sbml_basename)
    if not files.file_check(sbml_file_destination):
        if files.file_check(sbml_file):
            shutil.copy2(sbml_file, sbml_file_destination)
        else:
            print(f"SBML file '{sbml_file}' not found.", file=sys.stderr)
            return
    else:
        print(f"SBML file '{sbml_file_destination}' already exists in the output directory.")
    
    #Move filter file to output directory if provided
    if filter_file:
        filter_basename = os.path.basename(filter_file)
        filter_file_destination = os.path.join(output_path, filter_basename)
        if not files.file_check(filter_file_destination):
            if files.file_check(filter_file):
                shutil.copy2(filter_file, filter_file_destination)
            else:
                print(f"Filter file '{filter_file}' not found.", file=sys.stderr)
                print(f'Proceeding without filter file.')
                filter_file_destination = None
        else:
            print(f"Filter file '{filter_file_destination}' already exists in the output directory.")
    else:
        filter_file_destination = None

    try:    
        programs.run_metagraphtools(output_path, sbml_file_destination, filter_file_destination, chokepoints=True, graph=True) 
        print(f'MetaGraphTools processing completed. Results are in {output_path}.')
        try:
            programs.change_permission_user_dir(output_path)
        except Exception as e:
            print(f"Error changing file permissions: {e}", file=sys.stderr)
    except Exception as e:
        print(f"Error running MetaGraphTools: {e}", file=sys.stderr)

def parse_mgt_results(mgt_results_dir):
    """
    Parse the results generated by MetaGraphTools.

    :param mgt_results_dir: Path to the MetaGraphTools results directory.

    :return: Tuple of dictionaries (consumption_chokepoint_dict, production_chokepoint_dict, betweenness_centrality_dict, degree_dict).
    
    """
    programs.change_permission_user_dir(mgt_results_dir)

    chokepoint_file = os.path.join(mgt_results_dir, 'chokepoint_genes.tsv')
    betweenness_centrality_file = os.path.join(mgt_results_dir, 'betweenness_centrality.tsv')
    
    consumption_chokepoint_dict = {}
    production_chokepoint_dict = {}
    betweenness_centrality_dict = {}
    degree_dict = {}
    
    # Parse chokepoint genes file
    if os.path.exists(chokepoint_file):
        try:
            chokepoint_df = pd.read_csv(chokepoint_file, sep='\t', usecols=['Gene', 'Consumption_Chokepoint', 'Production_Chokepoint'])
            
            # Convert boolean columns
            for col in ['Consumption_Chokepoint', 'Production_Chokepoint']:
                if col in chokepoint_df.columns:
                    chokepoint_df[col] = chokepoint_df[col].astype(bool)
            
            # Create dictionaries with Gene as key and boolean values
            for _, row in chokepoint_df.iterrows():
                gene = row['Gene']
                if row['Consumption_Chokepoint']:
                    consumption_chokepoint_dict[gene] = True
                if row['Production_Chokepoint']:
                    production_chokepoint_dict[gene] = True
            
            print(f'Parsed {len(chokepoint_df)} chokepoint gene entries.')
            print(f'Consumption chokepoints: {len(consumption_chokepoint_dict)}')
            print(f'Production chokepoints: {len(production_chokepoint_dict)}')
            
        except Exception as e:
            print(f'Error parsing chokepoint genes file: {e}', file=sys.stderr)
    else:
        print(f'Chokepoint genes file not found: {chokepoint_file}', file=sys.stderr)
    
    # Parse betweenness centrality file
    if os.path.exists(betweenness_centrality_file):
        try:
            betweenness_df = pd.read_csv(betweenness_centrality_file, sep='\t', usecols=['Gene', 'Betweenness_Centrality', 'Degree'])
            
            # Ensure numeric columns are proper floats/ints
            if 'Betweenness_Centrality' in betweenness_df.columns:
                betweenness_df['Betweenness_Centrality'] = betweenness_df['Betweenness_Centrality'].astype(float)
            if 'Degree' in betweenness_df.columns:
                betweenness_df['Degree'] = betweenness_df['Degree'].astype(int)
            
            # Create dictionaries with Gene as key

            betweenness_centrality_dict = betweenness_df.groupby('Gene')['Betweenness_Centrality'].max().to_dict()
            degree_dict = betweenness_df.groupby('Gene')['Degree'].max().to_dict()

            print(f'Parsed {len(betweenness_df)} betweenness centrality entries.')
            
        except Exception as e:
            print(f'Error parsing betweenness centrality file: {e}', file=sys.stderr)
    else:
        print(f'Betweenness centrality file not found: {betweenness_centrality_file}', file=sys.stderr)
    
    return consumption_chokepoint_dict, production_chokepoint_dict, betweenness_centrality_dict, degree_dict



def run_metabolism_sbml (base_path, organism_name, sbml_file, filter_file):

    metabolism_dir = os.path.join(base_path, 'organism', organism_name, 'metabolism')

    print(f'---------- Processing external SBML file with MetaGraphTools ----------')

    # Find MGT_results* folder in metabolism_dir and remove it

    for item in os.listdir(metabolism_dir):
        if item.startswith('MGT_results'):
            mgt_results_dir = os.path.join(metabolism_dir, item)
            if os.path.isdir(mgt_results_dir):
                shutil.rmtree(mgt_results_dir)
                print(f'Removed existing MGT_results folder: {mgt_results_dir}')
    
    try:
        process_external_sbml(sbml_file, metabolism_dir, filter_file)
        counter = 0
        for item in os.listdir(metabolism_dir):
            if item.startswith('MGT_results'):
                counter += 1
                mgt_results_dir = os.path.join(metabolism_dir, item)
        if counter == 0:
            print('MetaGraphTools results folder not found.', file=sys.stderr)
            return
        elif counter > 1:
            print('Multiple MetaGraphTools results folders found. Please ensure only one exists.', file=sys.stderr)
            return
        else:
            consumption_chokepoint_dict, production_chokepoint_dict, betweenness_centrality_dict, degree_dict = parse_mgt_results(mgt_results_dir)

            bcentrality_df = metadata.metadata_table_with_values(base_path, organism_name, betweenness_centrality_dict, "MGT_betweenness_centrality", metabolism_dir, 0)
            degree_df = metadata.metadata_table_with_values(base_path, organism_name, degree_dict, "MGT_edges", metabolism_dir, 0)
            consumption_df = metadata.metadata_table_with_values(base_path, organism_name, consumption_chokepoint_dict, "MGT_consuming_chokepoints", metabolism_dir, 'False')
            production_df = metadata.metadata_table_with_values(base_path, organism_name, production_chokepoint_dict, "MGT_producing_chokepoints", metabolism_dir, 'False')                                                                

            print(f'---------- Finished processing external SBML file ----------')
    
            return bcentrality_df, degree_df, consumption_df, production_df
        
    except Exception as e:
        print(f'Error processing external SBML file: {e}', file=sys.stderr)

    